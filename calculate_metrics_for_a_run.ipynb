{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:24.387928Z",
     "start_time": "2025-01-17T03:33:24.375297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "a03d32b1ca9a396f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:26.416343Z",
     "start_time": "2025-01-17T03:33:24.392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from calculate_metric import get_stats_for_state\n",
    "from visualize_actor import get_state_traj\n"
   ],
   "id": "8b024d1e9cfcad0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:36.286430Z",
     "start_time": "2025-01-17T03:33:26.474489Z"
    }
   },
   "source": [
    "artifact_version = \"381\"\n",
    "num_episodes = 100\n",
    "model_artifact_remote_name = (\n",
    "    f\"josssdan/JaxInforMARL/PPO_RNN_Runner_State:v{artifact_version}\"\n",
    ")\n",
    "\n",
    "traj_batch, config, env = get_state_traj(model_artifact_remote_name, artifact_version, num_episodes)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   11 of 11 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'derived_values': {'minibatch_size': 76725,\n",
      "                    'num_actors': 300,\n",
      "                    'num_updates': 29,\n",
      "                    'scaled_clip_eps': 0.2},\n",
      " 'env_config': {'env_cls_name': 'TargetMPEEnvironment',\n",
      "                'env_kwargs': {'agent_communication_type': None,\n",
      "                               'agent_control_noise_std': 0.0,\n",
      "                               'agent_max_speed': -1,\n",
      "                               'agent_visibility_radius': [100],\n",
      "                               'collision_reward': -5,\n",
      "                               'entities_initial_coord_radius': [1],\n",
      "                               'entity_acceleration': 5,\n",
      "                               'max_steps': 100,\n",
      "                               'num_agents': 3,\n",
      "                               'one_time_death_reward': 5}},\n",
      " 'network_config': {'actor_num_hidden_linear_layer': 2,\n",
      "                    'critic_num_hidden_linear_layer': 2,\n",
      "                    'entity_type_embedding_dim': 3,\n",
      "                    'fc_dim_size': 64,\n",
      "                    'graph_attention_key_dim': 16,\n",
      "                    'graph_hidden_feature_dim': 16,\n",
      "                    'graph_num_linear_layer': 2,\n",
      "                    'gru_hidden_dim': 64,\n",
      "                    'num_graph_attn_layers': 2,\n",
      "                    'num_heads_per_attn_layer': 3},\n",
      " 'training_config': {'anneal_lr': True,\n",
      "                     'gamma': 0.99,\n",
      "                     'lr': 0.001,\n",
      "                     'num_envs': 100,\n",
      "                     'num_seeds': 2,\n",
      "                     'ppo_config': {'clip_eps': 0.2,\n",
      "                                    'entropy_coefficient': 0.01,\n",
      "                                    'gae_lambda': 0.95,\n",
      "                                    'is_clip_eps_per_env': False,\n",
      "                                    'max_grad_norm': 10,\n",
      "                                    'num_minibatches_actors': 8,\n",
      "                                    'num_steps_per_update': 2046,\n",
      "                                    'update_epochs': 10,\n",
      "                                    'value_coefficient': 0.5},\n",
      "                     'seed': 65,\n",
      "                     'total_timesteps': 6000000.0},\n",
      " 'wandb_config': {'checkpoint_model_every_update_steps': 100.0,\n",
      "                  'entity': 'josssdan',\n",
      "                  'mode': 'online',\n",
      "                  'project': 'JaxInforMARL',\n",
      "                  'save_model': True}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josdan/miniconda3/envs/InforMARLJAX/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:37.159589Z",
     "start_time": "2025-01-17T03:33:37.133865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_envs = config.training_config.num_envs\n",
    "num_agents = config.env_config.env_kwargs.num_agents\n",
    "num_steps = config.env_config.env_kwargs.max_steps"
   ],
   "id": "243953c079fafd50",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:37.954657Z",
     "start_time": "2025-01-17T03:33:37.174869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshaping so that the axis becomes num_env, num_steps, num_agents...\n",
    "\n",
    "traj_batch = jax.tree.map(lambda x: x.reshape(num_steps, num_agents, num_envs, *x.shape[2:]), traj_batch)\n",
    "traj_batch = jax.tree.map(\n",
    "    lambda x: jnp.swapaxes(x, 1, 2),\n",
    "    traj_batch,\n",
    ")\n",
    "traj_batch = jax.tree.map(\n",
    "    lambda x: jnp.swapaxes(x, 0, 1),\n",
    "    traj_batch,\n",
    ")\n"
   ],
   "id": "be320bcf7a38ca12",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.012623Z",
     "start_time": "2025-01-17T03:33:37.988439Z"
    }
   },
   "cell_type": "code",
   "source": "jax.tree.map(lambda x: x.shape, traj_batch)",
   "id": "c14560665810f9dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransitionWithEnvState(global_done=(100, 100, 3), done=(100, 100, 3), action=(100, 100, 3), value=(100, 100, 3), reward=(100, 100, 3), log_prob=(100, 100, 3), obs=(100, 100, 3, 6), graph=GraphsTupleWithAgentIndex(nodes=(100, 100, 3, 6, 7), edges=(100, 100, 3, 21, 1), receivers=(100, 100, 3, 21), senders=(100, 100, 3, 21), globals=None, n_node=(100, 100, 3), n_edge=(100, 100, 3), agent_indices=(100, 100, 3)), world_state=(100, 100, 3, 18), info={'returned_episode': (100, 100, 3), 'returned_episode_lengths': (100, 100, 3), 'returned_episode_returns': (100, 100, 3)}, env_state=LogEnvState(env_state=MPEState(dones=(100, 100, 3, 3), step=(100, 100, 3), entity_positions=(100, 100, 3, 6, 2), entity_velocities=(100, 100, 3, 6, 2), did_agent_die_this_time_step=(100, 100, 3, 3), agent_communication_message=(100, 100, 3, 0), agent_visibility_radius=(100, 100, 3, 3)), episode_returns=(100, 100, 3, 3), episode_lengths=(100, 100, 3, 3), returned_episode_returns=(100, 100, 3, 3), returned_episode_lengths=(100, 100, 3, 3)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.113634Z",
     "start_time": "2025-01-17T03:33:38.046510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# summing across all steps in episode and across all agents\n",
    "total_reward = jnp.sum(traj_batch.reward, axis=(1, 2))\n",
    "avg_reward_per_episode = jnp.average(total_reward).item()"
   ],
   "id": "5825d20a7ead14fa",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.171243Z",
     "start_time": "2025-01-17T03:33:38.148584Z"
    }
   },
   "cell_type": "code",
   "source": "avg_reward_per_episode",
   "id": "e4dbf6617c2c421c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789.3582153320312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.519611Z",
     "start_time": "2025-01-17T03:33:38.221059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "done = jnp.swapaxes(traj_batch.done, 1, 2)  # so that it becomes num_env, num_agents, num_steps\n",
    "avg_goal_reach_time_in_episode_fraction = (jnp.argmax(done, axis=-1) + 1) / num_steps\n",
    "agents_that_didnt_reach_goal = jnp.all(~done, axis=-1)\n",
    "avg_goal_reach_time_in_episode_fraction = avg_goal_reach_time_in_episode_fraction.at[agents_that_didnt_reach_goal].set(\n",
    "    1)\n",
    "avg_goal_reach_time_in_episode_fraction = jnp.average(avg_goal_reach_time_in_episode_fraction).item()"
   ],
   "id": "a86b2b58613252a6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.579734Z",
     "start_time": "2025-01-17T03:33:38.556155Z"
    }
   },
   "cell_type": "code",
   "source": "avg_goal_reach_time_in_episode_fraction",
   "id": "59382adea20bab0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39000004529953003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.715042Z",
     "start_time": "2025-01-17T03:33:38.615550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reached_goal = jnp.any(done, axis=-1)\n",
    "all_agents_reached_goal = jnp.all(reached_goal, axis=-1)\n",
    "\n",
    "episode_percent_all_agents_reached_goals = jnp.average(all_agents_reached_goal) * 100\n",
    "episode_percent_all_agents_reached_goals = episode_percent_all_agents_reached_goals.item()"
   ],
   "id": "ec21d308c41dda7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.771083Z",
     "start_time": "2025-01-17T03:33:38.749017Z"
    }
   },
   "cell_type": "code",
   "source": "episode_percent_all_agents_reached_goals",
   "id": "e5055b7e305080ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.999998092651367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:38.830040Z",
     "start_time": "2025-01-17T03:33:38.806951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def compute_stats_for_all_episode(env, state):\n",
    "    compute_stats_for_every_step = jax.vmap(get_stats_for_state, in_axes=(None, 0))\n",
    "    compute_all_stats = jax.vmap(compute_stats_for_every_step, in_axes=(None, 0))\n",
    "    return compute_all_stats(env, state)"
   ],
   "id": "9ce79b5e0ff9a12",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:39.069593Z",
     "start_time": "2025-01-17T03:33:38.864249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_state = traj_batch.env_state.env_state\n",
    "env_state = jax.tree.map(lambda x: x[:, :, 0],\n",
    "                         env_state)  # take state from one agent since it will be the same for all agents"
   ],
   "id": "d73db91942908241",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:39.191434Z",
     "start_time": "2025-01-17T03:33:39.106687Z"
    }
   },
   "cell_type": "code",
   "source": "num_collisions, num_agent_died = compute_stats_for_all_episode(env, env_state)",
   "id": "8463dc1c85054829",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:39.297643Z",
     "start_time": "2025-01-17T03:33:39.231345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_num_collision_across_all_episodes = jnp.average(num_collisions).item()\n",
    "avg_num_deaths_across_all_episodes = jnp.average(num_agent_died).item()"
   ],
   "id": "57a3ae75722ef040",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T03:33:39.359135Z",
     "start_time": "2025-01-17T03:33:39.335754Z"
    }
   },
   "cell_type": "code",
   "source": "avg_reward_per_episode, avg_goal_reach_time_in_episode_fraction, f\"{episode_percent_all_agents_reached_goals} %\", avg_num_collision_across_all_episodes",
   "id": "224b5df8a8442adc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789.3582153320312,\n",
       " 0.39000004529953003,\n",
       " '29.999998092651367 %',\n",
       " 0.02159999869763851)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

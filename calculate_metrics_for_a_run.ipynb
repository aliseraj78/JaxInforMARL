{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:26.775605Z",
     "start_time": "2025-01-17T22:07:26.702145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "a03d32b1ca9a396f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:26.872200Z",
     "start_time": "2025-01-17T22:07:26.844703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from calculate_metric import get_stats_for_state\n",
    "from visualize_actor import get_state_traj\n"
   ],
   "id": "8b024d1e9cfcad0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:31.273561Z",
     "start_time": "2025-01-17T22:07:26.912527Z"
    }
   },
   "source": [
    "artifact_version = \"399\"\n",
    "num_episodes = 100\n",
    "model_artifact_remote_name = (\n",
    "    f\"josssdan/JaxInforMARL/PPO_RNN_Runner_State:v{artifact_version}\"\n",
    ")\n",
    "\n",
    "traj_batch, config, env = get_state_traj(model_artifact_remote_name, artifact_version, num_episodes)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:   11 of 11 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'derived_values': {'minibatch_size': 19200,\n",
      "                    'num_actors': 300,\n",
      "                    'num_updates': 78,\n",
      "                    'scaled_clip_eps': 0.2},\n",
      " 'env_config': {'env_cls_name': 'TargetMPEEnvironment',\n",
      "                'env_kwargs': {'agent_communication_type': None,\n",
      "                               'agent_control_noise_std': 0.0,\n",
      "                               'agent_max_speed': -1,\n",
      "                               'agent_visibility_radius': [0.5],\n",
      "                               'collision_reward': -1,\n",
      "                               'entities_initial_coord_radius': [1],\n",
      "                               'entity_acceleration': 5,\n",
      "                               'max_steps': 100,\n",
      "                               'num_agents': 3,\n",
      "                               'one_time_death_reward': 10}},\n",
      " 'network_config': {'actor_num_hidden_linear_layer': 2,\n",
      "                    'critic_num_hidden_linear_layer': 2,\n",
      "                    'entity_type_embedding_dim': 4,\n",
      "                    'fc_dim_size': 64,\n",
      "                    'graph_attention_key_dim': 16,\n",
      "                    'graph_hidden_feature_dim': 16,\n",
      "                    'graph_num_linear_layer': 2,\n",
      "                    'gru_hidden_dim': 64,\n",
      "                    'num_graph_attn_layers': 1,\n",
      "                    'num_heads_per_attn_layer': 3},\n",
      " 'training_config': {'anneal_lr': True,\n",
      "                     'gamma': 0.99,\n",
      "                     'lr': 0.0005,\n",
      "                     'num_envs': 100,\n",
      "                     'num_seeds': 2,\n",
      "                     'ppo_config': {'clip_eps': 0.2,\n",
      "                                    'entropy_coefficient': 0.01,\n",
      "                                    'gae_lambda': 0.95,\n",
      "                                    'is_clip_eps_per_env': False,\n",
      "                                    'max_grad_norm': 0.5,\n",
      "                                    'num_minibatches_actors': 4,\n",
      "                                    'num_steps_per_update': 256,\n",
      "                                    'update_epochs': 4,\n",
      "                                    'value_coefficient': 0.5},\n",
      "                     'seed': 65,\n",
      "                     'total_timesteps': 2000000.0},\n",
      " 'wandb_config': {'checkpoint_model_every_update_steps': 100.0,\n",
      "                  'entity': 'josssdan',\n",
      "                  'mode': 'online',\n",
      "                  'project': 'JaxInforMARL',\n",
      "                  'save_model': True}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josdan/miniconda3/envs/InforMARLJAX/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:31.811807Z",
     "start_time": "2025-01-17T22:07:31.789077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_envs = config.training_config.num_envs\n",
    "num_agents = config.env_config.env_kwargs.num_agents\n",
    "num_steps = config.env_config.env_kwargs.max_steps"
   ],
   "id": "243953c079fafd50",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:31.876990Z",
     "start_time": "2025-01-17T22:07:31.832602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshaping so that the axis becomes num_env, num_steps, num_agents...\n",
    "\n",
    "traj_batch = jax.tree.map(lambda x: x.reshape(num_steps, num_agents, num_envs, *x.shape[2:]), traj_batch)\n",
    "traj_batch = jax.tree.map(\n",
    "    lambda x: jnp.swapaxes(x, 1, 2),\n",
    "    traj_batch,\n",
    ")\n",
    "traj_batch = jax.tree.map(\n",
    "    lambda x: jnp.swapaxes(x, 0, 1),\n",
    "    traj_batch,\n",
    ")\n"
   ],
   "id": "be320bcf7a38ca12",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:31.942549Z",
     "start_time": "2025-01-17T22:07:31.919427Z"
    }
   },
   "cell_type": "code",
   "source": "jax.tree.map(lambda x: x.shape, traj_batch)",
   "id": "c14560665810f9dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransitionWithEnvState(global_done=(100, 100, 3), done=(100, 100, 3), action=(100, 100, 3), value=(100, 100, 3), reward=(100, 100, 3), log_prob=(100, 100, 3), obs=(100, 100, 3, 6), graph=GraphsTupleWithAgentIndex(nodes=(100, 100, 3, 6, 7), edges=(100, 100, 3, 21, 1), receivers=(100, 100, 3, 21), senders=(100, 100, 3, 21), globals=None, n_node=(100, 100, 3), n_edge=(100, 100, 3), agent_indices=(100, 100, 3)), world_state=(100, 100, 3, 18), info={'returned_episode': (100, 100, 3), 'returned_episode_lengths': (100, 100, 3), 'returned_episode_returns': (100, 100, 3)}, env_state=LogEnvState(env_state=MPEState(dones=(100, 100, 3, 3), step=(100, 100, 3), entity_positions=(100, 100, 3, 6, 2), entity_velocities=(100, 100, 3, 6, 2), did_agent_die_this_time_step=(100, 100, 3, 3), agent_communication_message=(100, 100, 3, 0), agent_visibility_radius=(100, 100, 3, 3)), episode_returns=(100, 100, 3, 3), episode_lengths=(100, 100, 3, 3), returned_episode_returns=(100, 100, 3, 3), returned_episode_lengths=(100, 100, 3, 3)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.005307Z",
     "start_time": "2025-01-17T22:07:31.982404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# summing across all steps in episode and across all agents\n",
    "total_reward = jnp.sum(traj_batch.reward, axis=(1, 2))\n",
    "avg_reward_per_episode = jnp.average(total_reward).item()"
   ],
   "id": "5825d20a7ead14fa",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.068559Z",
     "start_time": "2025-01-17T22:07:32.045533Z"
    }
   },
   "cell_type": "code",
   "source": "avg_reward_per_episode",
   "id": "e4dbf6617c2c421c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1910.878662109375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.288135Z",
     "start_time": "2025-01-17T22:07:32.107910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "done = jnp.swapaxes(traj_batch.done, 1, 2)  # so that it becomes num_env, num_agents, num_steps\n",
    "avg_goal_reach_time_in_episode_fraction = (jnp.argmax(done, axis=-1) + 1) / num_steps\n",
    "agents_that_didnt_reach_goal = jnp.all(~done, axis=-1)\n",
    "avg_goal_reach_time_in_episode_fraction = avg_goal_reach_time_in_episode_fraction.at[agents_that_didnt_reach_goal].set(\n",
    "    1)\n",
    "avg_goal_reach_time_in_episode_fraction = jnp.average(avg_goal_reach_time_in_episode_fraction).item()"
   ],
   "id": "a86b2b58613252a6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.349545Z",
     "start_time": "2025-01-17T22:07:32.327300Z"
    }
   },
   "cell_type": "code",
   "source": "avg_goal_reach_time_in_episode_fraction",
   "id": "59382adea20bab0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3938000798225403"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.410035Z",
     "start_time": "2025-01-17T22:07:32.387794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reached_goal = jnp.any(done, axis=-1)\n",
    "all_agents_reached_goal = jnp.all(reached_goal, axis=-1)\n",
    "\n",
    "episode_percent_all_agents_reached_goals = jnp.average(all_agents_reached_goal) * 100\n",
    "episode_percent_all_agents_reached_goals = episode_percent_all_agents_reached_goals.item()"
   ],
   "id": "ec21d308c41dda7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.473737Z",
     "start_time": "2025-01-17T22:07:32.450773Z"
    }
   },
   "cell_type": "code",
   "source": "episode_percent_all_agents_reached_goals",
   "id": "e5055b7e305080ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.536857Z",
     "start_time": "2025-01-17T22:07:32.512767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def compute_stats_for_all_episode(env, state):\n",
    "    compute_stats_for_every_step = jax.vmap(get_stats_for_state, in_axes=(None, 0))\n",
    "    compute_all_stats = jax.vmap(compute_stats_for_every_step, in_axes=(None, 0))\n",
    "    return compute_all_stats(env, state)"
   ],
   "id": "9ce79b5e0ff9a12",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.600140Z",
     "start_time": "2025-01-17T22:07:32.576182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_state = traj_batch.env_state.env_state\n",
    "env_state = jax.tree.map(lambda x: x[:, :, 0],\n",
    "                         env_state)  # take state from one agent since it will be the same for all agents"
   ],
   "id": "d73db91942908241",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.723914Z",
     "start_time": "2025-01-17T22:07:32.639739Z"
    }
   },
   "cell_type": "code",
   "source": "num_collisions, num_agent_died = compute_stats_for_all_episode(env, env_state)",
   "id": "8463dc1c85054829",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.787591Z",
     "start_time": "2025-01-17T22:07:32.764650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_num_collision_across_all_episodes = jnp.average(num_collisions).item()\n",
    "avg_num_deaths_across_all_episodes = jnp.average(num_agent_died).item()"
   ],
   "id": "57a3ae75722ef040",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T22:07:32.850619Z",
     "start_time": "2025-01-17T22:07:32.827114Z"
    }
   },
   "cell_type": "code",
   "source": "avg_reward_per_episode, avg_goal_reach_time_in_episode_fraction, f\"{episode_percent_all_agents_reached_goals} %\", avg_num_collision_across_all_episodes",
   "id": "224b5df8a8442adc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1910.878662109375, 0.3938000798225403, '36.0 %', 0.035349998623132706)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
